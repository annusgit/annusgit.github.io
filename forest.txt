Title: Monitoring Forest Cover Change Patterns Using Fully Convolutional Network Over Billion Tree Tsunami Afforestation Project Regions, Pakistan

The importance of forests on Earth cannot be emphasized enough. Forests cover roughly 30% of the Earth’s surface and are home to 80% of terrestrial species of animals, plants and insects. However, due to human development activities and natural disasters, these forests and ecosystems associated with them are under significant pressure of degradation and desertification. Around 2.3 million square kilometers of world’s forest cover has been lost between 2000 and 2012 causing not only financial losses but also severely damaging associated biodiversity and ecosystems. Unlike developed countries, the developing countries do not have the resources or budgets to conduct manual surveys to monitor forest change trends for long periods and hence they rely on estimations. We have developed a reliable method based on local data for proper quantification of forest cover change by using open source tools and techniques, which will help developing countries in more accurate estimation of the forests in order to counter challenges like climate change. 

Pakistan is among the worst hit countries by forest degradation and deforestation and if appropriate measures are not taken, the country is likely to lose all of its forests in the next thirty to
fifty years. To overcome this problem, the provincial government of Khyber Pakhtunkhwa (KP) initiated
the Billion Tree Tsunami (BTT) afforestation project. We have developed a satellite based image
analysis approach to quantitatively monitor the forest cover change as a result of BTT afforestation in KP
districts of Pakistan. The method is based on State-of-the-Art deep convolution neural networks based semantic
segmentation models. The analysis is completely data driven and has been performed using Freely Available Landsat-8 satellite imagery showing an average forest cover improvement of around 39% from 2014 to 2018. The change detection results and deforestation/afforestation hotspots identified during our analysis provide important insights into the forest change trends that will help in filling gaps in policy making that is essential for ensuring sustainable management of forests in Pakistan.

Study Area:

Khyber Pakhtunkhwa (KP) is one of the five provinces in the country and has the highest land area covered with forest at provincial level. In 2014, the BTT Project was launched in response to the international Bonn Challenge by the KP government in Pakistan. The Bonn Challenge aims to restore 150 million hectares of degraded and deforested land in the world by 2020 and 350 million hectares by 2030. It was initiated by the German government and the International Union for conservation of Nature (IUCN) in 2011 and was later extended by New York declaration on Forest at the 2014 UN Climate Summit. In total, there are 25 districts in which the BTT project afforestation drives have been carried out.

Our work focused on 17 out of 25 BTT districts that had a considerable forest cover to begin with and included Hangu, Karak, Kohat, Nowshehra, Battagram, Abbottabad, Kohistan, Haripur, Tor Ghar, Mansehra, Buner, Chitral, Lower Dir, Malakand, Shangla, Swat and Upper Dir.

Deep Learning:

Firstly, deep neural networks are defined by layers and operators, such as 2D and 3D convolutions, linear dot products, etc., and they extract features from the training data automatically and hence hand-crafted features are not needed as is the case with most statistical models. This results in a high-level of abstraction in designing neural network architectures and modern deep learning frameworks such as Pytorch and Tensorflow allow such networks to be designed quickly. Secondly, deep neural networks can be fine tuned, therefore pre-trained neural network layers that were trained on other similar datasets may be used in designing newer architectures since these layers learn must faster from newer datasets and allow the whole model to converge more quickly. Our forest cover segmentation model is based on the popular deep neural network model UNet and is implemented in Pytorch framework.

A UNet is called a "U-Net" because of its U-shaped architecture [54]. The first half of the network is an encoder, a fully convolutional feature extractor network like the VGG [69] but its classification head (dot products and softmax) is removed. An encoder may be viewed as multiple modules stacked one after the other such that the input of a module feeds fromthe output of the previous one. The operators used in all modules of this encoder are CONV, BatchNorm, ReLU, andMax POOL. Four of such modules comprise the encoder of our UNet, and it is known as a four-stage encoder. The other half of the UNet, the decoder, is the mirror image of the first half, a four-stage decoder. Each module in a decoder consists of Transposed CONV operation followed by a copy-and-fuse connection between the corresponding stages of encoder and decoder. It is followed by CONVs along with BatchNorm, ReLU andMax Pool. This concatenation operation of encoder and decoder outputs allows the decoder to utilize the features extracted by the encoder at subsequent stages. The encoder (dotted square on the left) is where the input tensor is downsampled and encoded into a smaller dimensional vector. The decoder (dotted square on the right) decodes this vector and produces full resolution segmentation for the input image. Each decoder module concatenates its output with the encoder output at the corresponding stage and the final decoder tensor is passed through a convolution again followed by a softmax layer which normalizes the output probabilities for both classes at each pixel.

Unet caption:

UNet architecture developed for our work. The encoder is the set of modules to the left and the decoder is the set of modules on the right hand side. The upsampling in the decoder is done by using a transposed convolution operator with a stride of 2. The arrows indicate tensor outputs from the encoder being copied and concatenated with the transposed convolution decoder output along the depth dimension. The input to this UNet is an 18 dimensional multispectral satellite image from the KP region.

Flow Chart:

Complete flow chart of the proposed pipeline for forest cover change analysis of BTT project. Set of all images in 2015 with less than 10% cloud cover are filtered and pixel wise median values are calculated for each band to generate a composite image. This image represent clean image of the year. The paper printed map is taken as a reference and forest/non-forest data points are labelled on this clean Landsat-8 image. This small set of data points is used to train a classifier that labels all the rest of the pixels in the clean Landsat-8 image. The result of this labelling is a digitized forest cover map. Then a Unet model is trained with clean Landsat-8 district images of 2015 as input and the target labels are the digitized forest cover maps of 2015. The trained model performs inference on the clean Landsat-8 district images of 2014, 2016, 2017 and 2018, yielding a temporal series of forest cover maps.